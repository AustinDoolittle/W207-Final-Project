{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W207 Final Project - Zillow Housing Data\n",
    "We examined a dataset from Zillow that was posted to Kaggle in 2017. The dataset includes many features from houses available in the Zillow catalogue. The task is to predict what the $logerror$ of the Zillow's estimate of the house's sale price ($Zestimate$) against the actual sale price of the house ($SalePrice$). The $logerror$ is defined as follows:\n",
    "\n",
    "$$ logerror = log(Zestimate) - log(SalePrice) $$\n",
    "\n",
    "The competition was evaluated on the submission's Mean Absolute Error($MAE$). This is calculated as follows:\n",
    "\n",
    "$$ MAE = \\frac{\\sum_{i=1}^n \\left|y_i - \\hat{y}_i\\right|}{n} $$\n",
    "\n",
    "Where $n$ is the number of entries in the dataset, $y_i$ is the actual value of the datapoint $i$, and $\\hat{y}_i$ is the predicted value of the datapoint $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import re as re\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numpy import errstate,isneginf\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters; register_matplotlib_converters()\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "import reverse_geocoder as rg # https://github.com/thampiman/reverse-geocoder\n",
    "import seaborn as sns\n",
    "sns.set(style = 'white')\n",
    "\n",
    "import gauntlet\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ehuang\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "train_2016 = pd.read_csv(\"train_2016_v2.csv\")\n",
    "properties_2016 = pd.read_csv(\"properties_2016.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we look at the housing features and label dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# look at properties 2016 dataframe\n",
    "properties_2016.describe(include='all').transpose().sort_values(by='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>parcelid</th>\n",
       "      <td>90275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.29847e+07</td>\n",
       "      <td>2.50451e+06</td>\n",
       "      <td>1.07117e+07</td>\n",
       "      <td>1.15595e+07</td>\n",
       "      <td>1.25473e+07</td>\n",
       "      <td>1.42276e+07</td>\n",
       "      <td>1.62961e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logerror</th>\n",
       "      <td>90275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0114572</td>\n",
       "      <td>0.161079</td>\n",
       "      <td>-4.605</td>\n",
       "      <td>-0.0253</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.0392</td>\n",
       "      <td>4.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transactiondate</th>\n",
       "      <td>90275</td>\n",
       "      <td>352</td>\n",
       "      <td>2016-07-29</td>\n",
       "      <td>910</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 count unique         top freq         mean          std  \\\n",
       "parcelid         90275    NaN         NaN  NaN  1.29847e+07  2.50451e+06   \n",
       "logerror         90275    NaN         NaN  NaN    0.0114572     0.161079   \n",
       "transactiondate  90275    352  2016-07-29  910          NaN          NaN   \n",
       "\n",
       "                         min          25%          50%          75%  \\\n",
       "parcelid         1.07117e+07  1.15595e+07  1.25473e+07  1.42276e+07   \n",
       "logerror              -4.605      -0.0253        0.006       0.0392   \n",
       "transactiondate          NaN          NaN          NaN          NaN   \n",
       "\n",
       "                         max  \n",
       "parcelid         1.62961e+08  \n",
       "logerror               4.737  \n",
       "transactiondate          NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at train 2016 dataframe\n",
    "train_2016.describe(include='all').transpose().sort_values(by='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine these two dataframes, joining on the `parcelid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the logerror and transactiondate from properties into train dataframe\n",
    "df = properties_2016.merge(train_2016, on='parcelid', how='inner',left_index=True,right_index=True, copy='False')\n",
    "df.set_index(keys='parcelid',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean\n",
    "First we do some minimal conversion of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the transactiondate column to a datetime\n",
    "df.transactiondate = pd.to_datetime(df.transactiondate, infer_datetime_format=True)\n",
    "\n",
    "# convert Lat/Long to actual Lat/Long\n",
    "# note - run this cell one-time or latitude/longtitude will keep dividing by 1e6\n",
    "df['latitude'] = df['latitude']/1e6\n",
    "df['longitude'] = df['longitude']/1e6\n",
    "\n",
    "# combine latitude and longtitude into one column\n",
    "df['latlng'] = df['latitude'].map(str) + ',' + df['longitude'].map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lat-Long Conversion\n",
    "Now we want to map the latitude and longitudes provided to cities and counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# lookup lat/long locations\n",
    "coordinates = df[\"latlng\"].apply(lambda x: tuple(x.split(',')))\n",
    "results = rg.search(coordinates.tolist())\n",
    "\n",
    "city = [result.get(\"name\") for result in results]\n",
    "county = [result.get(\"admin2\") for result in results]\n",
    "\n",
    "df['city'] = city\n",
    "df['county'] = county\n",
    "\n",
    "# create a combined variable for better categorization\n",
    "df['county_city'] = list(zip(county, city))\n",
    "df['county_city'] = df['county_city'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zillow kaggle page suggests that it only provides data from the Los Angeles, Orange, and Ventura counties in California. We can see that there are some values outside of these counties. We elect to filter these out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los Angeles County    59641\n",
      "Orange County         22972\n",
      "Ventura County         7535\n",
      "Name: county, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df[df['county'].isin(['Los Angeles County', 'Orange County', 'Ventura County'])]\n",
    "print(df['county'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed both categorical and continuous columns in the dataset, which we need to analyze differently to explore their effect on the logerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify categorical versus continuous columns \n",
    "categorical_columns = ['airconditioningtypeid','architecturalstyletypeid','buildingclasstypeid',\n",
    "                       'buildingqualitytypeid', 'heatingorsystemtypeid', 'latitude', 'longitude',\n",
    "                       'propertylandusetypeid','rawcensustractandblock','regionidcity','regionidcounty',\n",
    "                       'regionidneighborhood','regionidzip','typeconstructiontypeid','censustractandblock',\n",
    "                       'bathroomcnt','bedroomcnt','assessmentyear', 'roomcnt', 'hashottuborspa',\n",
    "                       'taxdelinquencyflag','fireplaceflag', 'city', 'county', 'county_city']\n",
    "\n",
    "continuous_columns = list(set(df.columns) - set(categorical_columns))\n",
    "# print(continuous_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - Speak to the plots below. Or maybe include all of these plots in the appendix and just reference interesting ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# feature count minus one due to \n",
    "feature_count = len(continuous_columns) - 1\n",
    "col_count = 3\n",
    "row_count = math.ceil(float(feature_count) / col_count)\n",
    "\n",
    "# print(col_count ,row_count, df.columns)\n",
    "fig, axes = plt.subplots(row_count, col_count, figsize = (20,24), dpi = 200)\n",
    "\n",
    "n_iter = 16\n",
    "axes = np.ravel(axes)\n",
    "for i, col_name in enumerate(continuous_columns):\n",
    "    ax = axes[i]\n",
    "    # beautiful plots referenced: https://imaddabbura.github.io/post/kmeans_clustering/\n",
    "    sns.scatterplot(x=col_name, y='logerror', data=df, hue = 'county', ax = ax)\n",
    "    ax.set_xlabel(col_name, fontsize = 8)\n",
    "    ax.set_ylabel('logerror', fontsize = 8)\n",
    "    ax.set_title('{0} vs logerror'.format(col_name), fontsize = 10)\n",
    "    ax.tick_params(axis='x', labelsize=6)\n",
    "    ax.legend(bbox_to_anchor=(1.0, 1), loc=2, borderaxespad=0.1, title_fontsize=8, fontsize=6)\n",
    "    ax.grid()\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "df.corr()['logerror'].sort_values(ascending = False)[1:].plot(kind='bar')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "#### Drop duplicative data\n",
    "\n",
    "We've decided to drop the following obviously duplicative columns whose information is better characterized by other variables present in the dataset. \n",
    "* fips - a five-digit code which identifies counties and county equivalents in the United States. Can be represented by city and county data pulled in\n",
    "* storytypeid -  type of floors in a multi-story house (i.e. basement and main level, split-level, attic, etc.) Can be represented by basementsqft\n",
    "* calculatedbathnbr - number of bathrooms in home including fractional bathroom. Can be represented by bathroomcnt and bedroomcnt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# drop fips, storytypeid, and calcuatedbathnbr\n",
    "df.drop(labels=['storytypeid','fips', 'calculatedbathnbr'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to character format\n",
    "Now we need to standardize some categorical data from characters to numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# standardize and convert data to 1 or 0\n",
    "def convert_yn(x):\n",
    "    if x in ['True','Y']:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "for col in ['hashottuborspa','taxdelinquencyflag','fireplaceflag']:\n",
    "    df[col] = df[col].astype('str').map(convert_yn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "#### Analyze continuous variables\n",
    "\n",
    "There are likely other variables that will not be helpful in improving model performance. We will first explore the continuous variables to understand their effect on logerror, before making a determination to keep or drop columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# filter only for columns with numeric values\n",
    "\n",
    "# some of these are actually categorical. Leave for Ray to analyze\n",
    "# EH - ideally we replace labels = [long list] with categorical_columns above for code simplicity...\n",
    "\n",
    "numeric_df = df.select_dtypes(include=['float64','int64']).drop(labels=['airconditioningtypeid','architecturalstyletypeid',\n",
    "                        'buildingclasstypeid','buildingqualitytypeid','heatingorsystemtypeid','latitude','longitude',\n",
    "                        'propertylandusetypeid','rawcensustractandblock','regionidcity','regionidcounty',\n",
    "                        'regionidneighborhood','regionidzip','typeconstructiontypeid','censustractandblock',\n",
    "                        'bathroomcnt','bedroomcnt','assessmentyear', 'roomcnt', 'hashottuborspa','taxdelinquencyflag',\n",
    "                        'fireplaceflag'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "def analyze_cols(df):\n",
    "    \"\"\" Takes a data frame and for each column calculates the mean and standard \n",
    "        deviation of absolute logerror for listings with this home populated \n",
    "        as well as without this home populated.\n",
    "        \n",
    "        Performs a two-tailed independent t-test to determine if the listings with \n",
    "        this column populated have a different logerror than those without it populated.\n",
    "        \n",
    "        Returns a dataframe with all values for each column\n",
    "    \"\"\"\n",
    "    out_list = []\n",
    "    for col in df.columns:\n",
    "        if col == \"logerror\":\n",
    "            continue\n",
    "            \n",
    "        my_dict = {}\n",
    "        \n",
    "        # Split the df based on whether the col is populated and just keep absolute value of logerror\n",
    "        pop_df = np.abs(df[(df[col].notna())]['logerror'])\n",
    "        nan_df = np.abs(df[(df[col].isnull())]['logerror'])\n",
    "\n",
    "        # Print out the mean and st dev logerror\n",
    "        my_dict[\"col\"] = col\n",
    "        my_dict[\"n\"] = pop_df.shape[0]\n",
    "        my_dict[\"mean_pop\"] = pop_df.mean()\n",
    "        my_dict[\"std_pop\"] = pop_df.std()\n",
    "        my_dict[\"mean_nan\"] = nan_df.mean()\n",
    "        my_dict[\"std_nan\"] = nan_df.std()\n",
    "        my_dict['p_val'] = stats.ttest_ind(pop_df,nan_df).pvalue\n",
    "        \n",
    "        out_list.append(my_dict)\n",
    "        \n",
    "    return pd.DataFrame(out_list)[['col','n','mean_pop','mean_nan','std_pop','std_nan','p_val']].sort_values(by=\"n\").set_index('col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>mean_pop</th>\n",
       "      <th>mean_nan</th>\n",
       "      <th>std_pop</th>\n",
       "      <th>std_nan</th>\n",
       "      <th>p_val</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>basementsqft</th>\n",
       "      <td>51</td>\n",
       "      <td>0.065247</td>\n",
       "      <td>0.068440</td>\n",
       "      <td>0.110930</td>\n",
       "      <td>0.146256</td>\n",
       "      <td>0.876114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yardbuildingsqft26</th>\n",
       "      <td>82</td>\n",
       "      <td>0.059695</td>\n",
       "      <td>0.068447</td>\n",
       "      <td>0.171019</td>\n",
       "      <td>0.146214</td>\n",
       "      <td>0.588057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finishedsquarefeet13</th>\n",
       "      <td>249</td>\n",
       "      <td>0.068342</td>\n",
       "      <td>0.068439</td>\n",
       "      <td>0.100739</td>\n",
       "      <td>0.146345</td>\n",
       "      <td>0.991657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decktypeid</th>\n",
       "      <td>572</td>\n",
       "      <td>0.077908</td>\n",
       "      <td>0.068378</td>\n",
       "      <td>0.158115</td>\n",
       "      <td>0.146158</td>\n",
       "      <td>0.120260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finishedsquarefeet6</th>\n",
       "      <td>658</td>\n",
       "      <td>0.058308</td>\n",
       "      <td>0.068513</td>\n",
       "      <td>0.099169</td>\n",
       "      <td>0.146526</td>\n",
       "      <td>0.074509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poolsizesum</th>\n",
       "      <td>905</td>\n",
       "      <td>0.070720</td>\n",
       "      <td>0.068415</td>\n",
       "      <td>0.161899</td>\n",
       "      <td>0.146071</td>\n",
       "      <td>0.637176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pooltypeid2</th>\n",
       "      <td>969</td>\n",
       "      <td>0.068871</td>\n",
       "      <td>0.068434</td>\n",
       "      <td>0.130858</td>\n",
       "      <td>0.146397</td>\n",
       "      <td>0.926259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pooltypeid10</th>\n",
       "      <td>1053</td>\n",
       "      <td>0.066811</td>\n",
       "      <td>0.068458</td>\n",
       "      <td>0.119985</td>\n",
       "      <td>0.146521</td>\n",
       "      <td>0.716339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>taxdelinquencyyear</th>\n",
       "      <td>1740</td>\n",
       "      <td>0.072253</td>\n",
       "      <td>0.068363</td>\n",
       "      <td>0.142779</td>\n",
       "      <td>0.146305</td>\n",
       "      <td>0.271946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yardbuildingsqft17</th>\n",
       "      <td>2769</td>\n",
       "      <td>0.073399</td>\n",
       "      <td>0.068281</td>\n",
       "      <td>0.155449</td>\n",
       "      <td>0.145935</td>\n",
       "      <td>0.069812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finishedsquarefeet15</th>\n",
       "      <td>5648</td>\n",
       "      <td>0.067469</td>\n",
       "      <td>0.068503</td>\n",
       "      <td>0.132968</td>\n",
       "      <td>0.147083</td>\n",
       "      <td>0.606700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finishedfloor1squarefeet</th>\n",
       "      <td>6920</td>\n",
       "      <td>0.073941</td>\n",
       "      <td>0.067981</td>\n",
       "      <td>0.157966</td>\n",
       "      <td>0.145212</td>\n",
       "      <td>0.001124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finishedsquarefeet50</th>\n",
       "      <td>6920</td>\n",
       "      <td>0.073941</td>\n",
       "      <td>0.067981</td>\n",
       "      <td>0.157966</td>\n",
       "      <td>0.145212</td>\n",
       "      <td>0.001124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threequarterbathnbr</th>\n",
       "      <td>9668</td>\n",
       "      <td>0.070884</td>\n",
       "      <td>0.068145</td>\n",
       "      <td>0.153646</td>\n",
       "      <td>0.145321</td>\n",
       "      <td>0.081879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fireplacecnt</th>\n",
       "      <td>9921</td>\n",
       "      <td>0.068962</td>\n",
       "      <td>0.068374</td>\n",
       "      <td>0.139306</td>\n",
       "      <td>0.147073</td>\n",
       "      <td>0.705397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pooltypeid7</th>\n",
       "      <td>14628</td>\n",
       "      <td>0.067877</td>\n",
       "      <td>0.068547</td>\n",
       "      <td>0.149115</td>\n",
       "      <td>0.145675</td>\n",
       "      <td>0.611933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poolcnt</th>\n",
       "      <td>15597</td>\n",
       "      <td>0.067939</td>\n",
       "      <td>0.068543</td>\n",
       "      <td>0.148043</td>\n",
       "      <td>0.145859</td>\n",
       "      <td>0.638886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numberofstories</th>\n",
       "      <td>21400</td>\n",
       "      <td>0.069377</td>\n",
       "      <td>0.068146</td>\n",
       "      <td>0.143401</td>\n",
       "      <td>0.147110</td>\n",
       "      <td>0.282311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>garagetotalsqft</th>\n",
       "      <td>27427</td>\n",
       "      <td>0.069345</td>\n",
       "      <td>0.068042</td>\n",
       "      <td>0.144759</td>\n",
       "      <td>0.146880</td>\n",
       "      <td>0.218564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>garagecarcnt</th>\n",
       "      <td>27427</td>\n",
       "      <td>0.069345</td>\n",
       "      <td>0.068042</td>\n",
       "      <td>0.144759</td>\n",
       "      <td>0.146880</td>\n",
       "      <td>0.218564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unitcnt</th>\n",
       "      <td>59113</td>\n",
       "      <td>0.068036</td>\n",
       "      <td>0.069205</td>\n",
       "      <td>0.146906</td>\n",
       "      <td>0.144957</td>\n",
       "      <td>0.254428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lotsizesquarefeet</th>\n",
       "      <td>81890</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.069229</td>\n",
       "      <td>0.144985</td>\n",
       "      <td>0.158137</td>\n",
       "      <td>0.606083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finishedsquarefeet12</th>\n",
       "      <td>82228</td>\n",
       "      <td>0.068622</td>\n",
       "      <td>0.066533</td>\n",
       "      <td>0.147571</td>\n",
       "      <td>0.131595</td>\n",
       "      <td>0.224718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fullbathcnt</th>\n",
       "      <td>86298</td>\n",
       "      <td>0.068413</td>\n",
       "      <td>0.069002</td>\n",
       "      <td>0.146352</td>\n",
       "      <td>0.143684</td>\n",
       "      <td>0.806946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>landtaxvaluedollarcnt</th>\n",
       "      <td>88312</td>\n",
       "      <td>0.068379</td>\n",
       "      <td>0.071317</td>\n",
       "      <td>0.146172</td>\n",
       "      <td>0.149398</td>\n",
       "      <td>0.394158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearbuilt</th>\n",
       "      <td>88646</td>\n",
       "      <td>0.068481</td>\n",
       "      <td>0.065908</td>\n",
       "      <td>0.146328</td>\n",
       "      <td>0.140860</td>\n",
       "      <td>0.498910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>structuretaxvaluedollarcnt</th>\n",
       "      <td>88686</td>\n",
       "      <td>0.068447</td>\n",
       "      <td>0.067919</td>\n",
       "      <td>0.146442</td>\n",
       "      <td>0.133332</td>\n",
       "      <td>0.891025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calculatedfinishedsquarefeet</th>\n",
       "      <td>88783</td>\n",
       "      <td>0.068471</td>\n",
       "      <td>0.066297</td>\n",
       "      <td>0.146274</td>\n",
       "      <td>0.143953</td>\n",
       "      <td>0.585652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>taxvaluedollarcnt</th>\n",
       "      <td>89080</td>\n",
       "      <td>0.068404</td>\n",
       "      <td>0.071356</td>\n",
       "      <td>0.146266</td>\n",
       "      <td>0.143925</td>\n",
       "      <td>0.511912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>taxamount</th>\n",
       "      <td>89533</td>\n",
       "      <td>0.068481</td>\n",
       "      <td>0.062270</td>\n",
       "      <td>0.146340</td>\n",
       "      <td>0.130518</td>\n",
       "      <td>0.293911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  n  mean_pop  mean_nan   std_pop   std_nan  \\\n",
       "col                                                                           \n",
       "basementsqft                     51  0.065247  0.068440  0.110930  0.146256   \n",
       "yardbuildingsqft26               82  0.059695  0.068447  0.171019  0.146214   \n",
       "finishedsquarefeet13            249  0.068342  0.068439  0.100739  0.146345   \n",
       "decktypeid                      572  0.077908  0.068378  0.158115  0.146158   \n",
       "finishedsquarefeet6             658  0.058308  0.068513  0.099169  0.146526   \n",
       "poolsizesum                     905  0.070720  0.068415  0.161899  0.146071   \n",
       "pooltypeid2                     969  0.068871  0.068434  0.130858  0.146397   \n",
       "pooltypeid10                   1053  0.066811  0.068458  0.119985  0.146521   \n",
       "taxdelinquencyyear             1740  0.072253  0.068363  0.142779  0.146305   \n",
       "yardbuildingsqft17             2769  0.073399  0.068281  0.155449  0.145935   \n",
       "finishedsquarefeet15           5648  0.067469  0.068503  0.132968  0.147083   \n",
       "finishedfloor1squarefeet       6920  0.073941  0.067981  0.157966  0.145212   \n",
       "finishedsquarefeet50           6920  0.073941  0.067981  0.157966  0.145212   \n",
       "threequarterbathnbr            9668  0.070884  0.068145  0.153646  0.145321   \n",
       "fireplacecnt                   9921  0.068962  0.068374  0.139306  0.147073   \n",
       "pooltypeid7                   14628  0.067877  0.068547  0.149115  0.145675   \n",
       "poolcnt                       15597  0.067939  0.068543  0.148043  0.145859   \n",
       "numberofstories               21400  0.069377  0.068146  0.143401  0.147110   \n",
       "garagetotalsqft               27427  0.069345  0.068042  0.144759  0.146880   \n",
       "garagecarcnt                  27427  0.069345  0.068042  0.144759  0.146880   \n",
       "unitcnt                       59113  0.068036  0.069205  0.146906  0.144957   \n",
       "lotsizesquarefeet             81890  0.068359  0.069229  0.144985  0.158137   \n",
       "finishedsquarefeet12          82228  0.068622  0.066533  0.147571  0.131595   \n",
       "fullbathcnt                   86298  0.068413  0.069002  0.146352  0.143684   \n",
       "landtaxvaluedollarcnt         88312  0.068379  0.071317  0.146172  0.149398   \n",
       "yearbuilt                     88646  0.068481  0.065908  0.146328  0.140860   \n",
       "structuretaxvaluedollarcnt    88686  0.068447  0.067919  0.146442  0.133332   \n",
       "calculatedfinishedsquarefeet  88783  0.068471  0.066297  0.146274  0.143953   \n",
       "taxvaluedollarcnt             89080  0.068404  0.071356  0.146266  0.143925   \n",
       "taxamount                     89533  0.068481  0.062270  0.146340  0.130518   \n",
       "\n",
       "                                 p_val  \n",
       "col                                     \n",
       "basementsqft                  0.876114  \n",
       "yardbuildingsqft26            0.588057  \n",
       "finishedsquarefeet13          0.991657  \n",
       "decktypeid                    0.120260  \n",
       "finishedsquarefeet6           0.074509  \n",
       "poolsizesum                   0.637176  \n",
       "pooltypeid2                   0.926259  \n",
       "pooltypeid10                  0.716339  \n",
       "taxdelinquencyyear            0.271946  \n",
       "yardbuildingsqft17            0.069812  \n",
       "finishedsquarefeet15          0.606700  \n",
       "finishedfloor1squarefeet      0.001124  \n",
       "finishedsquarefeet50          0.001124  \n",
       "threequarterbathnbr           0.081879  \n",
       "fireplacecnt                  0.705397  \n",
       "pooltypeid7                   0.611933  \n",
       "poolcnt                       0.638886  \n",
       "numberofstories               0.282311  \n",
       "garagetotalsqft               0.218564  \n",
       "garagecarcnt                  0.218564  \n",
       "unitcnt                       0.254428  \n",
       "lotsizesquarefeet             0.606083  \n",
       "finishedsquarefeet12          0.224718  \n",
       "fullbathcnt                   0.806946  \n",
       "landtaxvaluedollarcnt         0.394158  \n",
       "yearbuilt                     0.498910  \n",
       "structuretaxvaluedollarcnt    0.891025  \n",
       "calculatedfinishedsquarefeet  0.585652  \n",
       "taxvaluedollarcnt             0.511912  \n",
       "taxamount                     0.293911  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_cols(numeric_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns\n",
    "\n",
    "Based on above statistical analysis, we decided to drop columns based on the following thresholds: \n",
    "* columns that are less than 5% populated (n < 4,507) \n",
    "* columns that have a p-value > 0.5\n",
    "\n",
    "The columns that meet both of these criteria are the following:\n",
    "* basementsqft\n",
    "* yardbuildingsqft26\n",
    "* finishedsquarefeet13\n",
    "* poolsizesum\n",
    "* pooltypeid2\n",
    "* pooltypeid10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that do not meet thresholds for influencing logerror\n",
    "df.drop(labels=['basementsqft', 'yardbuildingsqft26', 'finishedsquarefeet13', \n",
    "                'poolsizesum', 'pooltypeid2', 'pooltypeid10'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Ray\n",
    "## Going to convert categorical columns now \n",
    "\n",
    "## Not converting the one's we've decided to drop: pooltypeid2, pooltypeid10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# # Should we consider just converting this to yes or no ? 1/0.  \n",
    "# 1\tCentral Yes\n",
    "# 2\tChilled Water No\n",
    "# 3\tEvaporative Cooler No\n",
    "# 4\tGeo Thermal? No\n",
    "# 5\tNone No\n",
    "# 6\tOther No\n",
    "# 7\tPackaged AC Unit Yes\n",
    "# 8\tPartial No\n",
    "# 9\tRefrigeration Yes?\n",
    "# 10\tVentilation No?\n",
    "# 11\tWall Unit Yes\n",
    "# 12\tWindow Unit Yes\n",
    "# 13\tYes Yes\n",
    "def cat_air(x):\n",
    "    try:\n",
    "        if x in [1,7,9,11,12,13]:\n",
    "            return 'Yes'\n",
    "        elif x in [2,3,4,5,6,8,10]:\n",
    "            return 'No'\n",
    "        else:\n",
    "            return 'Blank'\n",
    "    except ValueError:\n",
    "        return 'Blank'\n",
    "combined_df['airconditioningtypeid'] = combined_df['airconditioningtypeid'].apply(cat_air)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(combined_df['airconditioningtypeid'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(combined_df.groupby(['county','airconditioningtypeid']).count()['county_city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HeatingOrSystemTypeID\tHeatingOrSystemDesc\n",
    "# 1\tBaseboard \n",
    "# 2\tCentral \n",
    "# 3\tCoal\n",
    "# 4\tConvection\n",
    "# 5\tElectric\n",
    "# 6\tForced air\n",
    "# 7\tFloor/Wall\n",
    "# 8\tGas\n",
    "# 9\tGeo Thermal\n",
    "# 10\tGravity\n",
    "# 11\tHeat Pump\n",
    "# 12\tHot Water\n",
    "# 13\tNone\n",
    "# 14\tOther\n",
    "# 15\tOil\n",
    "# 16\tPartial\n",
    "# 17\tPropane\n",
    "# 18\tRadiant\n",
    "# 19\tSteam\n",
    "# 20\tSolar\n",
    "# 21\tSpace/Suspended\n",
    "# 22\tVent\n",
    "# 23\tWood Burning\n",
    "# 24\tYes\n",
    "# 25\tZone\n",
    "def cat_heat(x):\n",
    "    try:\n",
    "        if x in [1,2,3,4,5,6,7,8,9,11,12,15,16,17,18,19,20,21,22,23,24,25]:\n",
    "            return 'Yes'\n",
    "        elif x in [10,13,14]:\n",
    "            return 'No'\n",
    "        else:\n",
    "            return 'Blank'\n",
    "    except ValueError:\n",
    "        return 'Blank'\n",
    "combined_df['heatingorsystemtypeid'] = combined_df['heatingorsystemtypeid'].apply(cat_heat)\n",
    "print(combined_df.groupby(['county','heatingorsystemtypeid']).count()['county_city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PropertyLandUseTypeID\tPropertyLandUseDesc\n",
    "# 31\tCommercial/Office/Residential Mixed Used\n",
    "# 46\tMulti-Story Store\n",
    "# 47\tStore/Office (Mixed Use)\n",
    "# 246\tDuplex (2 Units, Any Combination)\n",
    "# 247\tTriplex (3 Units, Any Combination)\n",
    "# 248\tQuadruplex (4 Units, Any Combination)\n",
    "# 260\tResidential General\n",
    "# 261\tSingle Family Residential\n",
    "# 262\tRural Residence\n",
    "# 263\tMobile Home\n",
    "# 264\tTownhouse\n",
    "# 265\tCluster Home\n",
    "# 266\tCondominium\n",
    "# 267\tCooperative\n",
    "# 268\tRow House\n",
    "# 269\tPlanned Unit Development\n",
    "# 270\tResidential Common Area\n",
    "# 271\tTimeshare\n",
    "# 273\tBungalow\n",
    "# 274\tZero Lot Line\n",
    "# 275\tManufactured, Modular, Prefabricated Homes\n",
    "# 276\tPatio Home\n",
    "# 279\tInferred Single Family Residential\n",
    "# 290\tVacant Land - General\n",
    "# 291\tResidential Vacant Land\n",
    "# def propertylandusetypeid(x):\n",
    "#     try:\n",
    "#         if x in [1,2,3,4,5,7,8,9,11,12,15,16,17,18,19,20,23,24]:\n",
    "#             return 'Yes'\n",
    "#         elif x in [6,10,13,14,21,22,25]:\n",
    "#             return 'No'\n",
    "#         else:\n",
    "#             return 'Blank'\n",
    "#     except ValueError:\n",
    "#         return 'Blank'\n",
    "# combined_df['propertylandusetypeid'] = combined_df['propertylandusetypeid'].apply(cat_heat)\n",
    "print(combined_df.groupby(['county','propertylandusetypeid']).count()['county_city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Covert this to high quality, medium, low?  (Range is from 1-12)\n",
    "\n",
    "def cat_qual(x):\n",
    "    try:\n",
    "        if x <=4:\n",
    "            return 'High'\n",
    "        elif x <=8:\n",
    "            return 'Medium'\n",
    "        else:\n",
    "            return 'Low'\n",
    "    except ValueError:\n",
    "        return 'Blank'\n",
    "combined_df['buildingqualitytypeid'] = combined_df['buildingqualitytypeid'].apply(cat_qual)\n",
    "\n",
    "print(combined_df.groupby(['county','buildingqualitytypeid']).count()['county_city'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to remove architecturalstyletypeid (170 records) and typeconstructiontypeid (201 records) and buildingclasstypeid (440 records)\n",
    "\n",
    "print(combined_df.groupby(['county','architecturalstyletypeid']).count()['county_city'])\n",
    "\n",
    "print(combined_df.groupby(['county','typeconstructiontypeid']).count()['county_city'])\n",
    "\n",
    "print(combined_df.groupby(['county','buildingclasstypeid']).count()['county_city'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns to drop: architecturalstyletypeid (170 records) and typeconstructiontypeid (201 records) and buildingclasstypeid (440 records). Drop latitude, longitude we have used it to map already. (Unless we want to try to use that somehow later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_df.groupby(['county','roomcnt']).count()['county_city'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will also drop census tract information. Doesn't tell us anything interesting. \n",
    "### Definition: A Census Block Group is a geographical unit used by the United States Census Bureau which is between the Census Tract and the Census Block. It is the smallest geographical unit for which the bureau publishes sample data, i.e. data which is only collected from a fraction of all households.\n",
    "\n",
    "### Fireplace flag only has 149 actual homes that have a fire place flag. Will that matter much? Keep for now. Idk what homes in CA even have fire places... lol\n",
    "\n",
    "### I'm going to remove the record with 86 rooms in OC. It's way too ridiculous. See above. Keep roomcnt as a variable, but make it exponential? not sure. Once standardized it might not matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(combined_df))\n",
    "combined_df = combined_df[combined_df['roomcnt'] != 86.0]\n",
    "print(len(combined_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_df.groupby(['county','bathroomcnt']).count()['county_city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_df.groupby(['county','bedroomcnt']).count()['county_city'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going to treat roomcnt, bedroomcnt, bathroomcnt as continuous and not remove them in categorical separation. PCA will probably handle that linear relationship b/w those rooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_df.groupby(['county','assessmentyear']).count()['county_city'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing assessment year, no useful information and dominantly 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['regionidcounty'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Below print just tells us total columns I should be analyzing to make sure I don't miss anything.\n",
    "##Can remove if everyone is okay w/ my work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_col = combined_df.columns\n",
    "numeric_col = numeric_df.columns\n",
    "missing_col = []\n",
    "for x in total_col:\n",
    "    if(x in numeric_df.columns):\n",
    "        None\n",
    "    else:\n",
    "        missing_col.append(x)\n",
    "        \n",
    "print(missing_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "## Save normalization/interoplation for after we analyze which columns have the most variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# interpolate missing values for the following columns\n",
    "#interpolate_columns = ['structuretaxvaluedollarcnt', 'taxvaluedollarcnt', \\\n",
    "#                      'landtaxvaluedollarcnt', 'taxamount', 'lotsizesquarefeet', \\\n",
    "#                      'garagetotalsqft']\n",
    "\n",
    "# default interpolation method is linear, let's just stick with that\n",
    "#for column in interpolate_columns:\n",
    "#    combined_df[column].interpolate(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# Commenting out for now - will normalize instead of using log transform\n",
    "# transform columns with very large st dev to log\n",
    "#log_transform_columns = ['structuretaxvaluedollarcnt', 'taxvaluedollarcnt', 'landtaxvaluedollarcnt', \\\n",
    "#                        'taxamount', 'lotsizesquarefeet']\n",
    "\n",
    "#eps = 1e-6\n",
    "#for column in log_transform_columns:\n",
    "#    combined_df[column] = np.log10(combined_df[column] + eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# normalize continuous variables\n",
    "#normalize_columns = ['calculatedfinishedsquarefeet', 'finishedsquarefeet12', 'garagetotalsqft', \\\n",
    "#                     'lotsizesquarefeet', 'poolsizesum', 'yardbuildingsqft17', \\\n",
    "#                    'structuretaxvaluedollarcnt', 'taxvaluedollarcnt', 'landtaxvaluedollarcnt',\n",
    "#                    'taxamount']\n",
    "\n",
    "\n",
    "#for column in normalize_columns:\n",
    "#    combined_df[column] = (combined_df[column] - combined_df[column].mean()) / combined_df[column].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "## Train Data EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "## Transaction Counts and Avg logerror by Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# plot logerrors over time\n",
    "plt_df = combined_df[['transactiondate','logerror']].copy()\n",
    "plt_df['transactiondate'] = plt_df.transactiondate#.astype('datetime64[M]')\n",
    "plt_df['logerror'] = np.abs(plt_df.logerror)\n",
    "g = plt_df.groupby(['transactiondate']).mean()\n",
    "g.plot()\n",
    "plt.title(\"Mean abs(logerror) vs Transaction Date\")\n",
    "plt.show()\n",
    "plt.hist(plt_df.transactiondate,bins=52)\n",
    "plt.title(\"Number of Transactions by Week\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "## <font color=red> This is the start of feature exploration, selection, and engineering... </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "## Split dataset into train, dev, and test and filter to July 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = ['airconditioningtypeid','buildingqualitytypeid', 'heatingorsystemtypeid', \n",
    "                       'propertylandusetypeid','taxdelinquencyflag', 'county']\n",
    "border_line = ['fireplaceflag','hashottuborspa']\n",
    "drop = ['architecturalstyletypeid','buildingclasstypeid','latitude', 'longitude', 'censustractandblock',\n",
    "        'typeconstructiontypeid', 'assessmentyear', 'regionidcity','regionidcounty','rawcensustractandblock',\n",
    "                        'regionidneighborhood','regionidzip', 'transactiondate', 'latlng', 'county_city', 'city', \n",
    "       'propertycountylandusecode','propertyzoningdesc' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# Set the columns to filter out of the dataset here\n",
    "excl_cols = ['basementsqft', 'yardbuildingsqft26', 'finishedsquarefeet13', 'poolsizesum', 'pooltypeid2', 'pooltypeid10',\n",
    "             'calculatedbathnbr']\n",
    "excl_cols = excl_cols + drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# split the dataset into train (60%), dev (20%), and test (20%)\n",
    "# Set a random seed (623) to keep our dataset consistent between runs\n",
    "# Only use data from July 2016\n",
    "\n",
    "df = shuffle(combined_df.copy(deep = True), random_state=623)\n",
    "if excl_cols is not None:\n",
    "    df.drop(labels = excl_cols, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['propertlandusetypeid'] = df_cat['propertylandusetypeid'].apply(str)\n",
    "df_dummies = pd.get_dummies(df, dummy_na = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_dummies.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df = df[(df['transactiondate'] >= '2016-07-01') & (df['transactiondate'] <= '2016-07-31')]\n",
    "\n",
    "i_logerror = df.columns.get_loc(\"logerror\")\n",
    "\n",
    "train_data, train_labels = df.iloc[:int(len(df)*0.6), np.r_[0:i_logerror, (i_logerror+1):len(df.columns)]], df.iloc[:int(len(df)*0.6),i_logerror:(i_logerror+1)].values\n",
    "df_train = df.iloc[:int(len(df)*0.6), :]\n",
    "dev_data, dev_labels = df.iloc[int(len(df)*0.6):int(len(df)*0.8), np.r_[0:i_logerror, (i_logerror+1):len(df.columns)]], df.iloc[int(len(df)*0.6):int(len(df)*0.8),i_logerror:(i_logerror+1)].values\n",
    "df_dev = df.iloc[int(len(df)*0.6):int(len(df)*0.8), :]\n",
    "test_data, test_labels = df.iloc[int(len(df)*0.8):, np.r_[0:i_logerror, (i_logerror+1):len(df.columns)]], df.iloc[int(len(df)*0.8):,i_logerror:(i_logerror+1)].values\n",
    "df_test = df.iloc[int(len(df)/2):, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "### Histograms of features in our train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# Plot bed/bath ratio vs logerror - any difference for outliers\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(df_train.bedroomcnt/df_train.bathroomcnt,df_train.logerror, alpha=0.2)\n",
    "plt.xlabel('Beds/Baths')\n",
    "plt.ylabel('Logerror')\n",
    "plt.title('Logerrors vs Bed/Bath ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# Plot beds and baths vs logerror - any difference for outliers\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(df_train[np.abs(df_train.logerror)>=0.1].bedroomcnt,df_train[np.abs(df_train.logerror)>=0.1].bathroomcnt,c=df_train[np.abs(df_train.logerror)>=0.1].logerror, cmap=plt.get_cmap('seismic'))\n",
    "plt.xlabel('Bedrooms')\n",
    "plt.ylabel('Bathrooms')\n",
    "plt.title('Logerrors more than +-0.1 by Beds/Baths')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# plot histograms of Logerrors\n",
    "fig, ax = plt.subplots(1,2, figsize=(14,6))\n",
    "\n",
    "ax.flatten()\n",
    "ax[0].hist(df_train.logerror,bins=40)\n",
    "ax[0].set_title(\"Logerror Histogram\")\n",
    "ax[1].hist(df_train.logerror[np.abs(df_train.logerror) >= 0.1],bins=40)\n",
    "ax[1].set_title(\"Logerror Histogram, excluding errors between +-0.1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# Plot histograms of the columns of interest that aren't binomial or way too many categories\n",
    "fig, ax = plt.subplots(8,5, figsize=(15,24))\n",
    "ax = ax.flatten()\n",
    "i = 0\n",
    "for c in df_train.iloc[:,0:48].columns:\n",
    "    if c in ['decktypeid','hashottuborspa','poolcnt','pooltypeid10','pooltypeid2','pooltypeid7','propertyzoningdesc',\n",
    "             'propertycountylandusecode','regionidcity','regionidcounty','regionidneighborhood','fireplaceflag',\n",
    "             'censustractandblock','latitude','longitude']:\n",
    "        continue\n",
    "    ax[i].hist(df_train[c])\n",
    "    ax[i].set_xlabel(\"\")\n",
    "    ax[i].set_ylabel(\"\")\n",
    "    ax[i].set_title(c)\n",
    "    i+=1\n",
    "\n",
    "plt.delaxes(ax[39])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "## <font color=green> EH - Trying dimensionality reduction (PCA) on subsets of data </font>\n",
    "We experiment with using PCA to group similar data (e.g. pool variables into a single dimension) then use the reduced projections moving forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# double check data types and shape\n",
    "# df_train.dtypes\n",
    "# df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# look at different groups of similar variables to understand if possible to combine\n",
    "sqft = df_train[[\"finishedfloor1squarefeet\", \"calculatedfinishedsquarefeet\",  \"finishedsquarefeet12\", \"finishedsquarefeet15\",\n",
    "                 \"finishedsquarefeet50\", \"finishedsquarefeet6\"]]\n",
    "\n",
    "garage = df_train[[\"garagecarcnt\", \"garagetotalsqft\"]]\n",
    "\n",
    "pool = df_train[[\"poolcnt\", \"pooltypeid7\"]]\n",
    "\n",
    "values = df_train[[\"structuretaxvaluedollarcnt\", \"taxvaluedollarcnt\", \"landtaxvaluedollarcnt\", \"taxamount\"]]\n",
    "\n",
    "region = df_train[[\"regionidcounty\", \"regionidcity\", \"regionidzip\", \"regionidneighborhood\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# replace NaN values with the mean of each column\n",
    "sqft = sqft.fillna(sqft.mean())\n",
    "garage = garage.fillna(garage.mean())\n",
    "pool = pool.fillna(pool.mean())\n",
    "values = values.fillna(values.mean())\n",
    "region = region.fillna(region.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# project grouped data into 1 dimension\n",
    "sqft_projected = PCA(n_components = 1).fit_transform(sqft)\n",
    "garage_projected = PCA(n_components = 1).fit_transform(garage)\n",
    "values_projected = PCA(n_components = 1).fit_transform(values)\n",
    "region_projected = PCA(n_components = 1).fit_transform(region)\n",
    "\n",
    "# probably can't project pool data because the replacement value is all 1, so thre is no point of comparison\n",
    "pool_projected = PCA(n_components = 1).fit_transform(pool)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "## PCA with un-transformed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "First, create a subset that excludes the non-numeric variables. PCA only works well on continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_train_data = train_data.select_dtypes(include=['float64','int64'])\n",
    "pca_train_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also exclude categorical variables, since PCA explains variance and a category ID of 9 is likely just as different from an ID of 2 as it is an ID of 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_train_data.drop(labels=['airconditioningtypeid','architecturalstyletypeid','buildingclasstypeid',\n",
    "                            'buildingqualitytypeid','decktypeid','heatingorsystemtypeid','propertylandusetypeid',\n",
    "                           'rawcensustractandblock','regionidcity','regionidcounty','regionidneighborhood',\n",
    "                           'regionidzip','typeconstructiontypeid','censustractandblock','pooltypeid7'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill all NaNs accordingly. Some will make sense to set to another column, or set to the mean; others should just be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These should be set to zero if they aren't populated - they likely don't exist\n",
    "for col in ['calculatedfinishedsquarefeet','fireplacecnt','fullbathcnt',\n",
    "           'garagecarcnt','garagetotalsqft','poolcnt','threequarterbathnbr',\n",
    "            'yardbuildingsqft17','taxdelinquencyyear']:\n",
    "    pca_train_data[col].fillna(0, inplace=True)\n",
    "    \n",
    "# Set unitcnt and number of stories to 1 if NaN\n",
    "pca_train_data['unitcnt'].fillna(1,inplace=True)\n",
    "pca_train_data['numberofstories'].fillna(1,inplace=True)\n",
    "\n",
    "# Fill these with calculatedfinishedsquarefeet if they don't exist\n",
    "for col in ['finishedfloor1squarefeet','finishedsquarefeet12','finishedsquarefeet15',\n",
    "           'finishedsquarefeet50','finishedsquarefeet6','lotsizesquarefeet']:\n",
    "    pca_train_data[col].fillna(pca_train_data['calculatedfinishedsquarefeet'], inplace=True)\n",
    "    \n",
    "# For the tax info, if the land value = total value, set structure NaN to 0 (it's just a lot)\n",
    "combined_df[(combined_df[\"taxvaluedollarcnt\"] == \n",
    "             combined_df[\"landtaxvaluedollarcnt\"])][[\"structuretaxvaluedollarcnt\"]].fillna(0, inplace=True)\n",
    "\n",
    "# Now set every other NaN to the mean, since we don't have a better guess\n",
    "for col in ['yearbuilt', 'structuretaxvaluedollarcnt','taxvaluedollarcnt',\n",
    "           'landtaxvaluedollarcnt','taxamount']:\n",
    "    pca_train_data[col].fillna(pca_train_data[col].mean(), inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now normalize the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in pca_train_data.columns:\n",
    "    pca_train_data[col] = (pca_train_data[col] - pca_train_data[col].mean()) / pca_train_data[col].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA for all components\n",
    "pca = PCA()\n",
    "pca.fit(pca_train_data) \n",
    "\n",
    "# Create a cumulative line plot of the explained variance ratio for each of the principal components\n",
    "plt.plot(pca.explained_variance_ratio_.cumsum())\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.xlabel('Principle component (k)')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like PCA with ~20 components retains nearly 100% of variance. This would reduce our number of features by 11. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 20).fit(pca_train_data)\n",
    "pca_20_train = pca.transform(pca_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "## Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(gauntlet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models on Ellie's PCA outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "def concat_tables(*args):\n",
    "    out_arr = []\n",
    "    for arg in args:\n",
    "        arg_np = np.array(arg)\n",
    "        out_arr.append(arg_np)\n",
    "        \n",
    "    return np.concatenate(out_arr, axis=1)\n",
    "\n",
    "full_data = concat_tables(sqft, garage, values, region)\n",
    "projected_data = concat_tables(sqft_projected, garage_projected, values_projected, region_projected)\n",
    "labels = df_train['logerror']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "gauntlet.compare_datasets(full_data, labels, projected_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "err_, err_std_ = gauntlet.cross_validate(MLPRegressor, full_data, labels)\n",
    "print(f'MLP Error: {err_:.3f}, Error std: {err_std_:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "It looks like the MLPRegressor is just bad. Maybe this could be improved via fiddling with the network architecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# first let's try our rule of thumb\n",
    "num_in_and_out = int((full_data.shape[1] + 1) * (2/3))\n",
    "err_, err_std_ = gauntlet.cross_validate(MLPRegressor, full_data, labels, hidden_layer_sizes=[num_in_and_out, 5], max_iter=1000)\n",
    "print(f'MLP Improved Error: {err_:.3f}, Error std: {err_std_:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "That's a definite improvement. What if we tried it with the smaller data? Note that we are keeping the network architecture consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "err_, err_std_ = gauntlet.cross_validate(MLPRegressor, projected_data, labels, hidden_layer_sizes=[num_in_and_out, 5], max_iter=1000)\n",
    "print(f'MLP Improved Error: {err_:.3f}, Error std: {err_std_:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "Nope, still pretty bad. Now let's take a look at Gradient Boosting Trees, as this appears to have performed pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gauntlet.viz_model(GradientBoostingRegressor, full_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "It is very interesting that the training score decreases sharply as the amount of training data increases. This appears to indicate noise in the training data. The other thing that is immediately noticable in the prediction error plot is how grouped together the errors are around 0. Let's see what happens with the reduced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())\n",
    "# gauntlet.viz_model(GradientBoostingRegressor, projected_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models with 20-component PCA model vs full numerical dataset (31 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauntlet.compare_datasets(pca_train_data, labels, pca_20_train, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try GBT with the 20-component PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauntlet.viz_model(GradientBoostingRegressor, pca_20_train, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauntlet.viz_model(GradientBoostingRegressor, pca_train_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "## t-SNE Clustering\n",
    "We perform t-SNE clustering to identify if there is a visible grouping of higher `logerror`s. While t-SNE is not great for hard analysis, it is a good visualization utility and would allow us to identify if there is a clear grouping of variables with larger logerrors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# remove categorical variables for now\n",
    "# AB - removed censustractandblock, rawcensustractandblock, and binary features (hashottuborspa, fireplaceflag, taxdelinquincyflag)\n",
    "tsne_train_columns = [\n",
    "       'bathroomcnt', 'bedroomcnt', 'calculatedbathnbr',\n",
    "       'finishedfloor1squarefeet', 'calculatedfinishedsquarefeet',\n",
    "       'finishedsquarefeet12', 'finishedsquarefeet15',\n",
    "       'finishedsquarefeet50', 'finishedsquarefeet6', 'fireplacecnt',\n",
    "       'fullbathcnt', 'garagecarcnt', 'garagetotalsqft',\n",
    "       'lotsizesquarefeet', 'poolcnt', 'roomcnt',\n",
    "       'threequarterbathnbr', 'unitcnt',\n",
    "       'yardbuildingsqft17', 'yearbuilt', \n",
    "       'structuretaxvaluedollarcnt', 'taxvaluedollarcnt', 'assessmentyear',\n",
    "       'landtaxvaluedollarcnt', 'taxamount', \n",
    "       'taxdelinquencyyear', 'logerror',\n",
    "       'transactiondate']\n",
    "tsne_train_df = df_train[tsne_train_columns].copy()\n",
    "\n",
    "tsne_train_df['transactiondate'] = tsne_train_df['transactiondate'].astype(np.int64)\n",
    "\n",
    "drop_columns = []\n",
    "for column in tsne_train_df.columns:\n",
    "    if (~tsne_train_df[column].isna()).sum() > 5000:\n",
    "        # Normalize all features here\n",
    "        if column != 'logerror':\n",
    "            tsne_train_df[column] = (tsne_train_df[column] - tsne_train_df[column].mean()) / tsne_train_df[column].std()\n",
    "        continue\n",
    "    \n",
    "    drop_columns.append(column)\n",
    "tsne_train_df.drop(drop_columns, axis='columns', inplace=True)\n",
    "\n",
    "\n",
    "#tsne_train_df['transactiondate'] = (tsne_train_df['transactiondate'] - tsne_train_df['transactiondate'].mean()) / tsne_train_df['transactiondate'].std()\n",
    "#tsne_train_df['assessmentyear'] = (tsne_train_df['assessmentyear'] - tsne_train_df['assessmentyear'].mean()) / tsne_train_df['assessmentyear'].std()\n",
    "tsne_train_df.dropna(axis='rows', inplace=True)\n",
    "\n",
    "tsna_train_np = tsne_train_df.loc[:,tsne_train_df.columns != 'logerror']\n",
    "### WARNING!!! THIS TAKES A WHILE!!!\n",
    "tsne_results = TSNE(n_components=2, verbose=1).fit_transform(tsna_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "plt.scatter(tsne_results[:,0], tsne_results[:,1], alpha = .8, c = np.abs(tsne_train_df['logerror']), cmap = 'inferno')\n",
    "cbar = plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "As we saw in the earlier `logerror` distribution, a very large portion of the logerrors are extremely close to 0, making the colormapping difficult to understand. Let's observe the values both in and outside of these limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# filter out high logerrors\n",
    "mask = np.abs(tsne_train_df['logerror']) < 0.1\n",
    "tsne_train_df_sub = tsne_train_df[mask]\n",
    "tsne_results_sub = tsne_results[mask]\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.scatter(tsne_results_sub[:,0], tsne_results_sub[:,1], alpha = .8, c = np.abs(tsne_train_df_sub['logerror']), cmap = 'inferno')\n",
    "cbar = plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# filter out low logerrors\n",
    "mask = np.abs(tsne_train_df['logerror']) > 0.1\n",
    "tsne_train_df_sub = tsne_train_df[mask]\n",
    "tsne_results_sub = tsne_results[mask]\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.scatter(tsne_results_sub[:,0], tsne_results_sub[:,1], alpha = .8, c = np.abs(tsne_train_df_sub['logerror']), cmap = 'inferno')\n",
    "cbar = plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "Unfortunately it doesn't look like there is a very large tendency for clusters to have similar logerror values, which means that this task will be pretty difficult. One very interesting thing to note is the linearity in the clusters that were created. Typically these clusters are both long and fat, but in this distribution it appears to be very tight and skinny. I'm not sure what this indicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# AB  - crazy thought - what if we take the log(logerror) to better visualize?\n",
    "#mask = np.abs(tsne_train_df['logerror']) < 0.1\n",
    "tsne_train_df_sub = tsne_train_df #[mask]\n",
    "tsne_results_sub = tsne_results #[mask]\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.scatter(tsne_results_sub[:,0], tsne_results_sub[:,1], alpha = .8, c = -np.log(np.abs(tsne_train_df_sub['logerror'])), cmap = 'inferno')\n",
    "cbar = plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
